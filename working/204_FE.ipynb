{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_hdf('C:/Users/f3107/Desktop/hy_data/train_002.h5')\n",
    "train['t'] = train['t'].dt.total_seconds().astype('int')\n",
    "\n",
    "test = pd.read_hdf('C:/Users/f3107/Desktop/hy_data/test_002.h5')\n",
    "test['t'] = test['t'].dt.total_seconds().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train.drop_duplicates('ship').loc[:,['ship','type']]\n",
    "test_label = test.drop_duplicates('ship').loc[:,'ship']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异常值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单行异常值剔除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v>20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['v']<20]\n",
    "test = test[test['v']<20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### xy异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_xy(df):\n",
    "    \n",
    "    # 生成时间间隔 d_t\n",
    "    df['d_t'] = df['t'].diff()\n",
    "    df.loc[0,'d_t'] = 0\n",
    "    df['d_t'] = df['d_t'].astype('int')\n",
    "\n",
    "    # 生成d_x, d_y\n",
    "    df['d_x'] = df['x'].diff()\n",
    "    df.loc[0,'d_x'] = 0\n",
    "    df['v_x'] = df['d_x']/df['d_t']\n",
    "    df.loc[0,'v_x'] = 0\n",
    "\n",
    "    df['d_y'] = df['y'].diff()\n",
    "    df.loc[0,'d_y'] = 0\n",
    "    df['v_y'] = df['d_y']/df['d_t']\n",
    "    df.loc[0,'v_y'] = 0\n",
    "    \n",
    "    df = df[(abs(df['d_x'])<200000) & (abs(df['d_y'])<200000)]\n",
    "    df = df[(abs(df['v_x'])<15) & (abs(df['v_y'])<15)]\n",
    "    df = df[~((abs(df['v_x'])>10) & (df['v']<3)) | ((abs(df['v_y'])>10) & (df['v']<3))]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = del_xy(train)\n",
    "train = del_xy(train)\n",
    "test = del_xy(test)\n",
    "test = del_xy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整体异常值/停泊状态优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 整体v，d归零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_range(train):\n",
    "\n",
    "    train_x = train['x'].groupby(train['ship']).agg(['max','min']).reset_index().rename(columns = {'max':'x_max','min':'x_min'})\n",
    "    train_y = train['y'].groupby(train['ship']).agg(['max','min']).reset_index().rename(columns = {'max':'y_max','min':'y_min'})\n",
    "    train_x['x_max_x_min'] = train_x['x_max'] - train_x['x_min']\n",
    "    train_y['y_max_y_min'] = train_y['y_max'] - train_y['y_min']\n",
    "\n",
    "    train_data = pd.merge(train_x, train_y, on ='ship')\n",
    "\n",
    "    ship_ID = list(train_data['ship'][(train_data['x_max_x_min']<100)&(train_data['y_max_y_min']<100)])\n",
    "\n",
    "    return ship_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_ID = xy_range(train)\n",
    "train['v'][train.ship.isin(ship_ID)] = 0\n",
    "train['d'][train.ship.isin(ship_ID)] = 0\n",
    "\n",
    "ship_ID = xy_range(test)\n",
    "test['v'][test.ship.isin(ship_ID)] = 0\n",
    "test['d'][test.ship.isin(ship_ID)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 时间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dt(df):\n",
    "\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['weekday'] = df['datetime'].dt.weekday\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = extract_dt(train)\n",
    "test = extract_dt(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_label = pd.merge(train_label, train.drop_duplicates('ship').loc[:,['ship', 'weekday']], on='ship')\n",
    "#test_label = pd.merge(test_label, test.drop_duplicates('ship').loc[:,['ship', 'weekday']], on='ship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max, min, mean, std, skew, sum, max-min, slope, area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_feature(df, key, target, aggs):   \n",
    "    agg_dict = {}\n",
    "    for ag in aggs:\n",
    "        agg_dict[f'{target}_{ag}'] = ag\n",
    "    #print(agg_dict)\n",
    "    t = df.groupby(key)[target].agg(agg_dict).reset_index()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(df, train):\n",
    "    \n",
    "    t = group_feature(df, 'ship','x',['max','min','mean','std','skew','sum','median']) \n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','y',['max','min','mean','std','skew','sum','median'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','v',['max','min','mean','std','skew','sum','median'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','d',['max','min','mean','std','skew','sum','median'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "\n",
    "   \n",
    "\n",
    "    '''\n",
    "    t = group_feature(df, 'ship','t',['max','min','mean','std','skew','sum'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','d_d',['max','min','mean','std','skew','sum'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','d_t',['max','min','mean','std','skew','sum'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','d_x',['max','min','mean','std','skew','sum'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','v_x',['max','min','mean','std','skew','sum'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','d_y',['max','min','mean','std','skew','sum'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','v_y',['max','min','mean','std','skew','sum'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    ''' \n",
    "     \n",
    "    train['x_max_x_min'] = train['x_max'] - train['x_min']\n",
    "    train['y_max_y_min'] = train['y_max'] - train['y_min']\n",
    "    train['y_max_x_min'] = train['y_max'] - train['x_min']\n",
    "    train['x_max_y_min'] = train['x_max'] - train['y_min']\n",
    "    \n",
    "    train['slope_1'] = train['y_max_y_min'] / np.where(train['x_max_x_min']==0, 0.001, train['x_max_x_min']) #d_y / d_x\n",
    "    train['slope_2'] = train['y_sum'] / np.where(train['x_sum']==0, 0.001, train['x_sum'])\n",
    "    \n",
    "    train['area'] = train['x_max_x_min'] * train['y_max_y_min']\n",
    "    \n",
    "    #小时值的统计量，取value_counts最多的那个\n",
    "    mode_hour = df.groupby('ship')['hour'].agg(lambda x:x.value_counts().index[0]).to_dict()\n",
    "    train['mode_hour'] = train['ship'].map(mode_hour)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_label = extract_feature(train, train_label)\n",
    "test_label = extract_feature(test, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整体相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_correlation(train, train_label):\n",
    "    corr = train[['x','y']].groupby(train['ship']).corr()\n",
    "    corr = corr.unstack()\n",
    "    c = corr['x']['y'].reset_index()\n",
    "    c.rename(columns={'y':'xy_corr'},inplace=True)\n",
    "    c = c.fillna(-99)\n",
    "    \n",
    "    train_label = pd.merge(train_label, c, on = 'ship')\n",
    "    \n",
    "    return train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_label = xy_correlation(train, train_label)\n",
    "#test_label = xy_correlation(test, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_cut(train):\n",
    "    \n",
    "    bins = [0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10, 20]\n",
    "    labels = [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10, 20]\n",
    "    \n",
    "    train['v_cut_0.5'] = pd.cut(train['v'],bins,labels = labels,include_lowest=True)\n",
    "    \n",
    "    #bins = [0, 0.5, 2, 3, 6, 20]\n",
    "    #labels = [0.5, 2, 3, 6, 20]\n",
    "    \n",
    "    #bins = [0, 0.7, 6.5, 20]\n",
    "    #labels = [0.7, 6.5, 20]\n",
    "    \n",
    "    bins = [0, 2, 6, 20]\n",
    "    labels = [2, 6, 20]\n",
    "    \n",
    "    train['v_cut_tuo'] = pd.cut(train['v'],bins,labels = labels,include_lowest=True)\n",
    "    \n",
    "    #bins = [0, 1.9, 5.3, 20]\n",
    "    #labels = [1.9, 5.3, 20]\n",
    "    \n",
    "    #train['v_cut_wei'] = pd.cut(train['v'],bins,labels = labels,include_lowest=True)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = v_cut(train)\n",
    "test = v_cut(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 频数统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_count_half(train, train_label):\n",
    "    \n",
    "    v_count = train['v_cut_0.5'].groupby([train['ship'], train['v_cut_0.5']]).count()\n",
    "    v_count = v_count.unstack()\n",
    "    v_count.columns = [v_count.columns.name+'_count_'+str(x) for x in v_count.columns.categories]\n",
    "    \n",
    "    v_count = v_count.fillna(0)\n",
    "    \n",
    "    v_count_sum = v_count.sum(axis=1)\n",
    "    v_count = v_count.div(v_count_sum, axis='rows')\n",
    "    \n",
    "    train_label = pd.merge(train_label, v_count, on = 'ship')\n",
    "    \n",
    "    return train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = v_count_half(train, train_label)\n",
    "test_label = v_count_half(test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_count_tuo(train, train_label):\n",
    "    \n",
    "    v_count = train['v_cut_tuo'].groupby([train['ship'], train['v_cut_tuo']]).count()\n",
    "    v_count = v_count.unstack()\n",
    "    v_count.columns = [v_count.columns.name+'_count_'+str(x) for x in v_count.columns.categories]\n",
    "    \n",
    "    v_count = v_count.fillna(0)\n",
    "    \n",
    "    v_count_sum = v_count.sum(axis=1)\n",
    "    v_count = v_count.div(v_count_sum, axis='rows')\n",
    "    \n",
    "    train_label = pd.merge(train_label, v_count, on = 'ship')\n",
    "    \n",
    "    return train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = v_count_tuo(train, train_label)\n",
    "test_label = v_count_tuo(test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_count_wei(train, train_label):\n",
    "    \n",
    "    v_count = train['v_cut_wei'].groupby([train['ship'], train['v_cut_wei']]).count()\n",
    "    v_count = v_count.unstack()\n",
    "    v_count.columns = [v_count.columns.name+'_count_'+str(x) for x in v_count.columns.categories]\n",
    "    \n",
    "    v_count = v_count.fillna(0)\n",
    "    \n",
    "    v_count_sum = v_count.sum(axis=1)\n",
    "    v_count = v_count.div(v_count_sum, axis='rows')\n",
    "    \n",
    "    train_label = pd.merge(train_label, v_count, on = 'ship')\n",
    "    \n",
    "    return train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_label = v_count_wei(train, train_label)\n",
    "#test_label = v_count_wei(test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_label.drop(['v_cut_tuo_count_2'], axis=1)\n",
    "test_label = test_label.drop(['v_cut_tuo_count_2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 切片相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vd_corr_tuo(train, train_label):\n",
    "    \n",
    "    vd_corr = train[['v','d']].groupby([train['ship'],train['v_cut_tuo']]).corr()\n",
    "    vd_corr_temp = vd_corr.unstack()\n",
    "    vd_corr_temp = vd_corr_temp['v']['d']\n",
    "    vd_corr = vd_corr_temp.unstack()\n",
    "    vd_corr.columns = [vd_corr.columns.name+'_corr_'+str(x) for x in vd_corr.columns.categories]\n",
    "    \n",
    "    vd_corr = vd_corr.fillna(-99)\n",
    "    \n",
    "    train_label = pd.merge(train_label, vd_corr, on = 'ship')\n",
    "    \n",
    "    return train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_label = vd_corr_tuo(train, train_label)\n",
    "#test_label = vd_corr_tuo(test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vd_corr_half(train, train_label):\n",
    "    \n",
    "    vd_corr = train[['v','d']].groupby([train['ship'],train['v_cut_0.5']]).corr()\n",
    "    vd_corr_temp = vd_corr.unstack()\n",
    "    vd_corr_temp = vd_corr_temp['v']['d']\n",
    "    vd_corr = vd_corr_temp.unstack()\n",
    "    vd_corr.columns = [vd_corr.columns.name+'_corr_'+str(x) for x in vd_corr.columns.categories]\n",
    "    \n",
    "    vd_corr = vd_corr.fillna(-99)\n",
    "    \n",
    "    train_label = pd.merge(train_label, vd_corr, on = 'ship')\n",
    "    \n",
    "    return train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_label = vd_corr_half(train, train_label)\n",
    "#test_label = vd_corr_half(test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_label = train_label.drop(['v_cut_0.5_corr_0.5'], axis=1)\n",
    "#test_label = test_label.drop(['v_cut_0.5_corr_0.5'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ship', 'type', 'x_max', 'x_min', 'x_mean', 'x_std', 'x_skew', 'x_sum',\n",
       "       'x_median', 'y_max', 'y_min', 'y_mean', 'y_std', 'y_skew', 'y_sum',\n",
       "       'y_median', 'v_max', 'v_min', 'v_mean', 'v_std', 'v_skew', 'v_sum',\n",
       "       'v_median', 'd_max', 'd_min', 'd_mean', 'd_std', 'd_skew', 'd_sum',\n",
       "       'd_median', 'x_max_x_min', 'y_max_y_min', 'y_max_x_min', 'x_max_y_min',\n",
       "       'slope_1', 'slope_2', 'area', 'mode_hour', 'v_cut_0.5_count_0.5',\n",
       "       'v_cut_0.5_count_1.0', 'v_cut_0.5_count_1.5', 'v_cut_0.5_count_2.0',\n",
       "       'v_cut_0.5_count_2.5', 'v_cut_0.5_count_3.0', 'v_cut_0.5_count_3.5',\n",
       "       'v_cut_0.5_count_4.0', 'v_cut_0.5_count_4.5', 'v_cut_0.5_count_5.0',\n",
       "       'v_cut_0.5_count_5.5', 'v_cut_0.5_count_6.0', 'v_cut_0.5_count_6.5',\n",
       "       'v_cut_0.5_count_7.0', 'v_cut_0.5_count_7.5', 'v_cut_0.5_count_8.0',\n",
       "       'v_cut_0.5_count_8.5', 'v_cut_0.5_count_9.0', 'v_cut_0.5_count_9.5',\n",
       "       'v_cut_0.5_count_10.0', 'v_cut_0.5_count_20.0', 'v_cut_tuo_count_6',\n",
       "       'v_cut_tuo_count_20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ship', 'x_max', 'x_min', 'x_mean', 'x_std', 'x_skew', 'x_sum',\n",
       "       'x_median', 'y_max', 'y_min', 'y_mean', 'y_std', 'y_skew', 'y_sum',\n",
       "       'y_median', 'v_max', 'v_min', 'v_mean', 'v_std', 'v_skew', 'v_sum',\n",
       "       'v_median', 'd_max', 'd_min', 'd_mean', 'd_std', 'd_skew', 'd_sum',\n",
       "       'd_median', 'x_max_x_min', 'y_max_y_min', 'y_max_x_min', 'x_max_y_min',\n",
       "       'slope_1', 'slope_2', 'area', 'mode_hour', 'v_cut_0.5_count_0.5',\n",
       "       'v_cut_0.5_count_1.0', 'v_cut_0.5_count_1.5', 'v_cut_0.5_count_2.0',\n",
       "       'v_cut_0.5_count_2.5', 'v_cut_0.5_count_3.0', 'v_cut_0.5_count_3.5',\n",
       "       'v_cut_0.5_count_4.0', 'v_cut_0.5_count_4.5', 'v_cut_0.5_count_5.0',\n",
       "       'v_cut_0.5_count_5.5', 'v_cut_0.5_count_6.0', 'v_cut_0.5_count_6.5',\n",
       "       'v_cut_0.5_count_7.0', 'v_cut_0.5_count_7.5', 'v_cut_0.5_count_8.0',\n",
       "       'v_cut_0.5_count_8.5', 'v_cut_0.5_count_9.0', 'v_cut_0.5_count_9.5',\n",
       "       'v_cut_0.5_count_10.0', 'v_cut_0.5_count_20.0', 'v_cut_tuo_count_6',\n",
       "       'v_cut_tuo_count_20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.to_hdf('C:/Users/f3107/Desktop/hy_data/train_204.h5', 'df', mode='w')\n",
    "test_label.to_hdf('C:/Users/f3107/Desktop/hy_data/test_204.h5', 'df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_label.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
